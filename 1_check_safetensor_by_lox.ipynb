{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading learned embeddings from:  logs/Pick_Images/transferable_identifier/ratio=0.9/dog1/2023-11-15T14-41-20_dog_baseline/lora_weight_s500.safetensors\n",
      "161\n",
      "dict_keys(['<krk1>', 'text_encoder:0:down', 'text_encoder:0:up', 'text_encoder:10:down', 'text_encoder:10:up', 'text_encoder:11:down', 'text_encoder:11:up', 'text_encoder:12:down', 'text_encoder:12:up', 'text_encoder:13:down', 'text_encoder:13:up', 'text_encoder:14:down', 'text_encoder:14:up', 'text_encoder:15:down', 'text_encoder:15:up', 'text_encoder:16:down', 'text_encoder:16:up', 'text_encoder:17:down', 'text_encoder:17:up', 'text_encoder:18:down', 'text_encoder:18:up', 'text_encoder:19:down', 'text_encoder:19:up', 'text_encoder:1:down', 'text_encoder:1:up', 'text_encoder:20:down', 'text_encoder:20:up', 'text_encoder:21:down', 'text_encoder:21:up', 'text_encoder:22:down', 'text_encoder:22:up', 'text_encoder:23:down', 'text_encoder:23:up', 'text_encoder:24:down', 'text_encoder:24:up', 'text_encoder:25:down', 'text_encoder:25:up', 'text_encoder:26:down', 'text_encoder:26:up', 'text_encoder:27:down', 'text_encoder:27:up', 'text_encoder:28:down', 'text_encoder:28:up', 'text_encoder:29:down', 'text_encoder:29:up', 'text_encoder:2:down', 'text_encoder:2:up', 'text_encoder:30:down', 'text_encoder:30:up', 'text_encoder:31:down', 'text_encoder:31:up', 'text_encoder:32:down', 'text_encoder:32:up', 'text_encoder:33:down', 'text_encoder:33:up', 'text_encoder:34:down', 'text_encoder:34:up', 'text_encoder:35:down', 'text_encoder:35:up', 'text_encoder:36:down', 'text_encoder:36:up', 'text_encoder:37:down', 'text_encoder:37:up', 'text_encoder:38:down', 'text_encoder:38:up', 'text_encoder:39:down', 'text_encoder:39:up', 'text_encoder:3:down', 'text_encoder:3:up', 'text_encoder:40:down', 'text_encoder:40:up', 'text_encoder:41:down', 'text_encoder:41:up', 'text_encoder:42:down', 'text_encoder:42:up', 'text_encoder:43:down', 'text_encoder:43:up', 'text_encoder:44:down', 'text_encoder:44:up', 'text_encoder:45:down', 'text_encoder:45:up', 'text_encoder:46:down', 'text_encoder:46:up', 'text_encoder:47:down', 'text_encoder:47:up', 'text_encoder:4:down', 'text_encoder:4:up', 'text_encoder:5:down', 'text_encoder:5:up', 'text_encoder:6:down', 'text_encoder:6:up', 'text_encoder:7:down', 'text_encoder:7:up', 'text_encoder:8:down', 'text_encoder:8:up', 'text_encoder:9:down', 'text_encoder:9:up', 'unet:0:down', 'unet:0:up', 'unet:10:down', 'unet:10:up', 'unet:11:down', 'unet:11:up', 'unet:12:down', 'unet:12:up', 'unet:13:down', 'unet:13:up', 'unet:14:down', 'unet:14:up', 'unet:15:down', 'unet:15:up', 'unet:16:down', 'unet:16:up', 'unet:17:down', 'unet:17:up', 'unet:18:down', 'unet:18:up', 'unet:19:down', 'unet:19:up', 'unet:1:down', 'unet:1:up', 'unet:20:down', 'unet:20:up', 'unet:21:down', 'unet:21:up', 'unet:22:down', 'unet:22:up', 'unet:23:down', 'unet:23:up', 'unet:24:down', 'unet:24:up', 'unet:25:down', 'unet:25:up', 'unet:26:down', 'unet:26:up', 'unet:27:down', 'unet:27:up', 'unet:28:down', 'unet:28:up', 'unet:29:down', 'unet:29:up', 'unet:2:down', 'unet:2:up', 'unet:30:down', 'unet:30:up', 'unet:31:down', 'unet:31:up', 'unet:3:down', 'unet:3:up', 'unet:4:down', 'unet:4:up', 'unet:5:down', 'unet:5:up', 'unet:6:down', 'unet:6:up', 'unet:7:down', 'unet:7:up', 'unet:8:down', 'unet:8:up', 'unet:9:down', 'unet:9:up'])\n"
     ]
    }
   ],
   "source": [
    "from lora_diffusion.lora import apply_learned_embed_in_clip, parse_safeloras_embeds\n",
    "from safetensors.torch import safe_open\n",
    "\n",
    "path = \"logs/Pick_Images/transferable_identifier/ratio=0.9/dog1/2023-11-15T14-41-20_dog_baseline/lora_weight_s500.safetensors\"\n",
    "\n",
    "print(\"Loading learned embeddings from: \", path)\n",
    "safeloras = safe_open(path, framework=\"pt\", device=\"cpu\")\n",
    "values = {}\n",
    "for key in safeloras.keys():\n",
    "    values[key] = safeloras.get_tensor(key)\n",
    "print(len(values.keys()))\n",
    "print(values.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0165,  0.0167, -0.0011, -0.0070, -0.0040,  0.0225, -0.0090, -0.0127,\n",
       "         0.0146, -0.0086,  0.0128,  0.0317,  0.0038, -0.0022,  0.0057])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values['<krk1>'].squeeze(0)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values['unet:10:up'].squeeze(0)[:8, 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
