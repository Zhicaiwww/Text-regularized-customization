{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! load_ext autoreload\n",
    "! autoreload 2\n",
    "from diffusers import StableDiffusionPipeline, EulerAncestralDiscreteScheduler\n",
    "import torch\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "from itertools import groupby\n",
    "from typing import Callable, Dict, List, Optional, Set, Tuple, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from lora_diffusion import LoraInjectedConv2d, LoraInjectedLinear, patch_pipe, tune_lora_scale, parse_safeloras\n",
    "from lora_diffusion.lora import _find_modules, UNET_CROSSATTN_TARGET_REPLACE, DEFAULT_TARGET_REPLACE\n",
    "from reg_lora.visual import visualize_images\n",
    "import safetensors\n",
    "\n",
    "# os.environ[\"DISABLE_TELEMETRY\"] = 'YES'\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "device = torch.device(\"cuda:3\")\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16,local_ckpt_files_only=True,revision='39593d5650112b4cc580433f6b0435385882d819').to(device)\n",
    "pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在attn_layer中插入hook函数, 可视化attn_map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/zhicai/poseVideo/lora-master/')\n",
    "path = 'lora_output/checkpoints/debug_decay_0.001/lora_weight_e31_s5000.safetensors'\n",
    "path = 'lora_output/checkpoints/output_dog_Ti-clip_Nonorm_3e-5/lora_weight_e62_s10000.safetensors'\n",
    "prompt = \"'a <krk1> dog in grand canyon'\"\n",
    "\n",
    "command = f\"python ../reg_lora/vis_attn_map.py --lora_path {path} --prompt {prompt} --gpu 0 --name up_blocks.2.attentions.1.transformer_blocks.0.attn2\"\n",
    "print(command)\n",
    "os.system(command)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算TI后cpation-image的Clip score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "lora_dir = '../lora_output/checkpoints/debug_decay_0.001'\n",
    "root_img_path = '/home/zhicai/poseVideo/Text-regularized-customization/custom_data/data/dog'\n",
    "ckpt_pattern = r'lora_weight_e\\d+_s\\d{3,4}.safetensors'\n",
    "captions = [\"photo of a <krk1> dog\", \"photo of a dog\"]\n",
    "device = 'cuda:2'\n",
    "weight_dtype = torch.float32\n",
    "\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device,dtype = weight_dtype)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "images = [Image.open(os.path.join(root_img_path, img_path)) for img_path in os.listdir(root_img_path)]\n",
    "    \n",
    "_ckpt_files = os.listdir(lora_dir)\n",
    "ckpt_files = [f for f in _ckpt_files if re.match(ckpt_pattern, f)]\n",
    "\n",
    "probs_list = []\n",
    "for _file in sorted(ckpt_files,\n",
    "                    key = lambda x: int(re.match(r'.*e([0-9]+).*', x.split('/')[-1])[1]), \n",
    "                    reverse=False):\n",
    "    \n",
    "    lora_path = os.path.join(lora_dir, _file)\n",
    "    print(f\"loading from: {lora_path}\")\n",
    "    patch_pipe( \n",
    "        pipe,\n",
    "        lora_path,\n",
    "        patch_text=False,\n",
    "        patch_ti=True,\n",
    "        patch_unet=True,\n",
    "        filter_crossattn_str = 'cross+self'\n",
    "    )\n",
    "\n",
    "    processor.tokenizer = pipe.tokenizer\n",
    "    model.text_model = copy.deepcopy(pipe.text_encoder.text_model.to(device, dtype=weight_dtype))\n",
    "    # token_embedding = model.text_model.embeddings.token_embedding\n",
    "    # token_embedding.weight.data[49408] = token_embedding.weight.data[42170]    \n",
    "    inputs = processor(text=captions, images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    image_embeds = outputs.image_embeds\n",
    "    input_ids = inputs.input_ids\n",
    "    last_hidden_state = outputs.text_model_output.last_hidden_state\n",
    "    text_embeds = last_hidden_state[torch.arange(last_hidden_state.size(0), device=input_ids.device),\n",
    "                                    [(row == 49407).nonzero().min() for row in input_ids]]\n",
    "    text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n",
    "    logits_per_image = logits_per_text.t()\n",
    "    \n",
    "    probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n",
    "    probs_list.append(probs[:,0].mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(np.arange(len(probs_list)) * 200, probs_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(np.arange(len(probs_list)) * 200, probs_list)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计Lora中的self和cross attn模块中的Linear-layer权重漂移和对文本映射后的语义漂移"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = '../output_dog'\n",
    "test_prompts = ['a dog','a cat','a teddybear','a chair']\n",
    "reg_prompt = ['photo of a dog']\n",
    "\n",
    "lora_weight_norm_list_cross = []\n",
    "lora_weight_norm_list_self = []\n",
    "lora_context_reg_resides_list = []\n",
    "\n",
    "# 找到cross_attn 和 self_attn的位置\n",
    "cnt = 0\n",
    "cross_idxs = []\n",
    "self_idxs = []\n",
    "for target, name, module in _find_modules(pipe.unet, DEFAULT_TARGET_REPLACE):\n",
    "    if module.in_features == 768:\n",
    "        cross_idxs.append(cnt)\n",
    "    if module.in_features != 320 and (name == 'to_k' or name == 'to_v'):\n",
    "        self_idxs.append(cnt)\n",
    "    cnt += 1 \n",
    "\n",
    "c_reg = pipe._encode_prompt(reg_prompt,device,1,False)\n",
    "group = [ckpt_file  for ckpt_file in os.listdir(target_dir) if '_s' in ckpt_file and 'lora_weight' in ckpt_file]\n",
    "print(group)\n",
    "sorted_group = sorted(group, key = lambda x: int(re.findall(r'.*s(\\d+).*',x)[0]))\n",
    "\n",
    "for ckpt_file in sorted_group:\n",
    "        cur_step_list_cross = []\n",
    "        cur_step_context_reg_reside = []\n",
    "        cur_step_list_self = []\n",
    "        ckpt = safetensors.safe_open(os.path.join(target_dir,ckpt_file), framework=\"pt\", device=\"cpu\")\n",
    "        lora = parse_safeloras(ckpt)  \n",
    "        for cross_idx , self_idx in zip(cross_idxs,self_idxs):\n",
    "                up_weight_self = lora['unet'][0][2*self_idx].to(device)\n",
    "                down_weight_self = lora['unet'][0][2*self_idx+1].to(device)\n",
    "                up_weight = lora['unet'][0][2*cross_idx].to(device)\n",
    "                down_weight = lora['unet'][0][2*cross_idx+1].to(device)\n",
    "                cur_step_lora_project_self = down_weight_self.transpose(0,1) @ up_weight_self.transpose(0,1)\n",
    "                cur_step_lora_project_cross = down_weight.transpose(0,1) @ up_weight.transpose(0,1)\n",
    "                cur_step_lora_reside = c_reg @ cur_step_lora_project_cross\n",
    "                cur_step_context_reg_reside.append(torch.norm(cur_step_lora_reside).cpu().item())\n",
    "                cur_step_list_cross.append(torch.norm(cur_step_lora_project_cross).cpu().item()) \n",
    "                cur_step_list_self.append(torch.norm(cur_step_lora_project_self).cpu().item())\n",
    "        lora_weight_norm_list_cross.append(np.mean(cur_step_list_cross))\n",
    "        lora_context_reg_resides_list.append(np.mean(cur_step_context_reg_reside))\n",
    "        lora_weight_norm_list_self.append(np.mean(cur_step_list_self))\n",
    "\n",
    "print(lora_weight_norm_list_cross[4:],'\\n',lora_weight_norm_list_self[4:],'\\n',lora_context_reg_resides_list[4:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "lora_ckpts = ['../lora_output/checkpoints/output_dog_Ti-clip_norm/lora_weight_e31_s5000.safetensors']\n",
    "prompts = ['photo of a <krk1> dog and a cat', 'photo of a cat and a <krk1> dog']\n",
    "bs = 4\n",
    "weight_dtype = torch.float16\n",
    "device = \"cuda:2\"\n",
    "pipe_copy = copy.deepcopy(pipe).to(device)\n",
    "torch.manual_seed(0)\n",
    "patch_pipe(\n",
    "    pipe_copy,\n",
    "    lora_ckpts[0],\n",
    "    patch_text=False,\n",
    "    patch_ti=True,\n",
    "    patch_unet=True,\n",
    "    filter_crossattn_str = 'cross+self'\n",
    ")\n",
    "# pipe.unet\n",
    "tune_lora_scale(pipe_copy.unet, 0)\n",
    "pipe_copy.text_encoder.text_model.to(device, dtype=weight_dtype)\n",
    "# tune_lora_scale(pipe_copy.text_encoder, 1)\n",
    "for prompt in prompts:\n",
    "    prompt = [prompt]*bs\n",
    "    img = pipe_copy(prompt = prompt, num_inference_steps=50, guidance_scale=6).images\n",
    "    visualize_images(img,outpath='figure/', nrow=4, show=True,save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "lora_ckpts = ['../lora_output/checkpoints/debug_decay_mask_0.001/lora_weight_e31_s5000.safetensors']\n",
    "prompts = ['photo of a <krk1> dog and a cat', 'photo of a cat and a <krk1> dog']\n",
    "bs = 4\n",
    "weight_dtype = torch.float16\n",
    "device = \"cuda:2\"\n",
    "pipe_copy = copy.deepcopy(pipe).to(device)\n",
    "torch.manual_seed(0)\n",
    "patch_pipe(\n",
    "    pipe_copy,\n",
    "    lora_ckpts[0],\n",
    "    patch_text=False,\n",
    "    patch_ti=True,\n",
    "    patch_unet=True,\n",
    "    filter_crossattn_str = 'cross+self'\n",
    ")\n",
    "# pipe.unet\n",
    "tune_lora_scale(pipe_copy.unet, 0)\n",
    "pipe_copy.text_encoder.text_model.to(device, dtype=weight_dtype)\n",
    "# tune_lora_scale(pipe_copy.text_encoder, 1)\n",
    "for prompt in prompts:\n",
    "    prompt = [prompt]*bs\n",
    "    img = pipe_copy(prompt = prompt, num_inference_steps=50, guidance_scale=6).images\n",
    "    visualize_images(img,outpath='figure/', nrow=4, show=True,save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "lora_ckpts = ['../lora_output/checkpoints/NT-clipDecay-mask']\n",
    "prompts = ['photo of a <krk1> dog', 'photo of a <krk1> dog swimming in a pool', 'photo of a <krk1> dog in grand canyon']\n",
    "bs = 4\n",
    "weight_dtype = torch.float16\n",
    "device = \"cuda:2\"\n",
    "pipe_copy = copy.deepcopy(pipe).to(device)\n",
    "torch.manual_seed(0)\n",
    "patch_pipe(\n",
    "    pipe_copy,\n",
    "    lora_ckpts[0],\n",
    "    patch_text=False,\n",
    "    patch_ti=True,\n",
    "    patch_unet=True,\n",
    "    filter_crossattn_str = 'cross+self'\n",
    ")\n",
    "# pipe.unet\n",
    "tune_lora_scale(pipe_copy.unet, 1)\n",
    "pipe_copy.text_encoder.text_model.to(device, dtype=weight_dtype)\n",
    "# tune_lora_scale(pipe_copy.text_encoder, 1)\n",
    "for prompt in prompts:\n",
    "    prompt = [prompt]*bs\n",
    "    img = pipe_copy(prompt = prompt, num_inference_steps=50, guidance_scale=6).images\n",
    "    visualize_images(img,outpath='figure/', nrow=4, show=True,save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "lora_ckpts = ['../lora_output/checkpoints/NT-Decay/lora_weight_e12_s2000.safetensors']\n",
    "prompts = ['photo of a <krk1> dog', 'photo of a <krk1> dog swimming in a pool']\n",
    "bs = 4\n",
    "weight_dtype = torch.float16\n",
    "device = \"cuda:2\"\n",
    "pipe_copy = copy.deepcopy(pipe).to(device)\n",
    "torch.manual_seed(0)\n",
    "patch_pipe(\n",
    "    pipe_copy,\n",
    "    lora_ckpts[0],\n",
    "    patch_text=False,\n",
    "    patch_ti=True,\n",
    "    patch_unet=True,\n",
    "    filter_crossattn_str = 'cross+self'\n",
    ")\n",
    "# pipe.unet\n",
    "tune_lora_scale(pipe_copy.unet, 1)\n",
    "pipe_copy.text_encoder.text_model.to(device, dtype=weight_dtype)\n",
    "# tune_lora_scale(pipe_copy.text_encoder, 1)\n",
    "for prompt in prompts:\n",
    "    prompt = [prompt]*bs\n",
    "    img = pipe_copy(prompt = prompt, num_inference_steps=50, guidance_scale=6).images\n",
    "    visualize_images(img,outpath='figure/', nrow=4, show=True,save=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked attention projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\n",
    "\n",
    "# print(input_ids)\n",
    "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n",
    "    \"\"\"\n",
    "    bsz, src_len = mask.size()\n",
    "    tgt_len = tgt_len if tgt_len is not None else src_len\n",
    "\n",
    "    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
    "\n",
    "    inverted_mask = 1.0 - expanded_mask\n",
    "    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n",
    "\n",
    "def get_identifier_masked_causal_attention_mask(bs, seq_len, identifier_indice, class_token_len, dtype = torch.float16):\n",
    "    mask = torch.empty(bs, seq_len, seq_len, dtype=dtype)\n",
    "    mask.fill_(torch.tensor(torch.finfo(dtype).min))\n",
    "    mask.triu_(1)  # zero out the lower diagonal\n",
    "    mask = mask.unsqueeze(1)  # expand mask\n",
    "    mask[:,:,identifier_indice, 1:max(identifier_indice,1)] = torch.finfo(dtype).min\n",
    "    mask[:,:,identifier_indice+class_token_len+1:,identifier_indice] = torch.finfo(dtype).min\n",
    "    return mask\n",
    "\n",
    "def build_causal_attention_mask( bs, seq_len, dtype):\n",
    "    mask = torch.empty(bs, seq_len, seq_len, dtype=dtype)\n",
    "    mask.fill_(torch.tensor(torch.finfo(dtype).min))\n",
    "    mask.triu_(1)  # zero out the lower diagonal\n",
    "    mask = mask.unsqueeze(1)  # expand mask\n",
    "    return mask\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_with_hidden_input(pipe, hidden_states ,num_images_per_prompt=1, num_inference_steps=50, guidance_scale=6):\n",
    "\n",
    "    text_embeddings = hidden_states\n",
    "    batch_size = hidden_states.size(0)\n",
    "    do_classifier_free_guidance = guidance_scale > 1.0\n",
    "    if do_classifier_free_guidance:\n",
    "        uncond_tokens = [\"\"] * batch_size\n",
    "        uncond_input = pipe.tokenizer(\n",
    "                uncond_tokens,\n",
    "                padding=\"max_length\",\n",
    "                max_length=pipe.tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "        uncond_embeddings = pipe.text_encoder(uncond_input.input_ids.to(device), attention_mask = None)[0]\n",
    "        bs_embed, seq_len, _ = text_embeddings.shape\n",
    "        text_embeddings = text_embeddings.repeat(1, num_images_per_prompt, 1)\n",
    "        text_embeddings = text_embeddings.view(bs_embed * num_images_per_prompt, seq_len, -1)\n",
    "        uncond_embeddings = uncond_embeddings.repeat(1, num_images_per_prompt, 1)\n",
    "        uncond_embeddings = uncond_embeddings.view(batch_size * num_images_per_prompt, seq_len, -1)\n",
    "        text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "    height =  pipe.unet.config.sample_size * pipe.vae_scale_factor\n",
    "    width = pipe.unet.config.sample_size * pipe.vae_scale_factor\n",
    "    pipe.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "    timesteps = pipe.scheduler.timesteps\n",
    "\n",
    "    # 5. Prepare latent variables\n",
    "    num_channels_latents = pipe.unet.in_channels\n",
    "    latents = pipe.prepare_latents(\n",
    "        batch_size * num_images_per_prompt,\n",
    "        num_channels_latents,\n",
    "        height,\n",
    "        width,\n",
    "        text_embeddings.dtype,\n",
    "        device,\n",
    "        None,\n",
    "        None,\n",
    "    )\n",
    "\n",
    "    # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n",
    "    extra_step_kwargs = pipe.prepare_extra_step_kwargs(None, 0.0)\n",
    "\n",
    "    # 7. Denoising loop\n",
    "    num_warmup_steps = len(timesteps) - num_inference_steps * pipe.scheduler.order\n",
    "    with pipe.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "        for i, t in enumerate(timesteps):\n",
    "            # expand the latents if we are doing classifier free guidance\n",
    "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "            latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "            # predict the noise residual\n",
    "            noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "            # perform guidance\n",
    "            if do_classifier_free_guidance:\n",
    "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "            # compute the previous noisy sample x_t -> x_t-1\n",
    "            latents = pipe.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n",
    "\n",
    "            # call the callback, if provided\n",
    "            if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % pipe.scheduler.order == 0):\n",
    "                progress_bar.update()\n",
    "\n",
    "    # 8. Post-processing\n",
    "    image = pipe.decode_latents(latents)\n",
    "\n",
    "    # 9. Run safety checker\n",
    "    # image, has_nsfw_concept = pipe.run_safety_checker(image, device, text_embeddings.dtype)\n",
    "    # 10. Convert to PIL\n",
    "    image = pipe.numpy_to_pil(image)\n",
    "\n",
    "    return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=None)\n",
    "\n",
    "\n",
    "inputs = pipe.tokenizer([\"photo of a red dog in grand canyon\"], return_tensors=\"pt\", max_length=77, truncation=True, padding='max_length')\n",
    "input_ids = inputs.input_ids.to(device)\n",
    "identifier_masked_causal_attention_mask = get_identifier_masked_causal_attention_mask(1, 77 ,4, 1, dtype= torch.float16).to(device)\n",
    "causal_attention_mask = build_causal_attention_mask(1, 77, dtype= torch.float16).to(device)\n",
    "clip_text_model = pipe.text_encoder.text_model\n",
    "hidden_states = clip_text_model.embeddings(input_ids=input_ids)\n",
    "outputs = clip_text_model.encoder(hidden_states,\n",
    "                    causal_attention_mask = identifier_masked_causal_attention_mask,\n",
    "                    # attention_mask = _expand_mask(attention_mask, torch.float16, 77).to(device),\n",
    "                    output_attentions=True)\n",
    "text_embedings = clip_text_model.final_layer_norm(outputs[0])\n",
    "print(text_embedings.size())\n",
    "imgs = generate_with_hidden_input(pipe, text_embedings, num_images_per_prompt = 4 , num_inference_steps=50, guidance_scale=6).images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_images(imgs,save=False,nrow=4,show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zhicai/miniconda3/envs/ldm/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda:2'\n",
    "a = torch.randn(1,2,3)\n",
    "b = torch.randn(1,2,3).to('cuda:0')\n",
    "for _inputs in [a, b]:\n",
    "    if isinstance(_inputs, torch.LongTensor):\n",
    "        _inputs = _inputs.to(device)\n",
    "    elif isinstance(_inputs, torch.FloatTensor):\n",
    "        _inputs = _inputs.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
