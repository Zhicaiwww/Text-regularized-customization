{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! load_ext autoreload\n",
    "! autoreload 2\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, EulerAncestralDiscreteScheduler\n",
    "import torch\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "from itertools import groupby\n",
    "from typing import Callable, Dict, List, Optional, Set, Tuple, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from lora_diffusion import LoraInjectedConv2d, LoraInjectedLinear, patch_pipe, tune_lora_scale, parse_safeloras\n",
    "from lora_diffusion.lora import _find_modules, UNET_CROSSATTN_TARGET_REPLACE, DEFAULT_TARGET_REPLACE\n",
    "from visualization.vis_image import visualize_images\n",
    "import safetensors\n",
    "import time\n",
    "\n",
    "os.environ[\"DISABLE_TELEMETRY\"] = 'YES'\n",
    "# os.environ[\"HTTP_PROXY\"] = \"http://localhost:7890\"\n",
    "# os.environ[\"HTTPS_PROXY\"] = \"http://localhost:7890\"\n",
    "model_id = \"/data/zhicai/code/Text-regularized-customization/models/stable-diffusion-v1-5\"\n",
    "device = torch.device(\"cuda:3\")\n",
    "time.sleep(0.01)\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16,revision='39593d5650112b4cc580433f6b0435385882d819',local_files_only=True).to(device)\n",
    "pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    model_id, torch_dtype=torch.float16, local_ckpt_files_only=True, revision='39593d5650112b4cc580433f6b0435385882d819').to(device)\n",
    "translation_pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(\n",
    "    translation_pipe.scheduler.config)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  可视化 Attention map\n",
    "在attn_layer中插入hook函数, 通过进行一轮采样，可视化对应层 name 中的attention map\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/zhicai/poseVideo/lora-master/')\n",
    "path = 'outputs/checkpoints/debug_decay_0.001/lora_weight_e31_s5000.safetensors'\n",
    "path = 'outputs/checkpoints/output_dog_Ti-clip_Nonorm_3e-5/lora_weight_e62_s10000.safetensors'\n",
    "prompt = \"'a <krk1> dog in grand canyon'\"\n",
    "\n",
    "command = f\"python ../reg_lora/vis_attn_map.py --lora_path {path} --prompt {prompt} --gpu 0 --name up_blocks.2.attentions.1.transformer_blocks.0.attn2\"\n",
    "print(command)\n",
    "os.system(command)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计 Textual inversion 之后的 CLIP Similarity\n",
    "这里统计的是 \"photo of a <krk1> dog\", \"photo of a dog\"的随着训练进程的CLIP相似度，同时\"photo of a <krk1> dog\"或者\"photo of a dog\"和训练图片随着训练进程的相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image and Caption 的 similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "# from custom_datasets import LIVE_SUBJECT_PROMPT_LIST\n",
    "\n",
    "target_dir = '/data/zhicai/code/Text-regularized-customization/outputs/dog/TiReg/TiReg_InitToken_weight_0_rank_10_seed_0'\n",
    "ckpt_pattern = r'lora_weight_s\\d{3,4}.safetensors'\n",
    "root_img_path = '../custom_datasets/data/dog'\n",
    "captions = [\"photo of a <krk1> dog swimming in a pool\",\n",
    "            \"photo of a <krk1> dog \",  \"photo of a dog swimming in a pool\"]\n",
    "weight_dtype = torch.float32\n",
    "\n",
    "# model = CLIPModel.from_pretrained(\n",
    "#     \"models/clip-vit-large-patch14\").to(device, dtype=weight_dtype)\n",
    "# processor = CLIPProcessor.from_pretrained(\"models/clip-vit-large-patch14\")\n",
    "\n",
    "images = [Image.open(os.path.join(root_img_path, img_path))\n",
    "          for img_path in os.listdir(root_img_path)]\n",
    "\n",
    "\n",
    "group = [ckpt_file for ckpt_file in os.listdir(\n",
    "    target_dir) if '_s' in ckpt_file and 'lora_weight' in ckpt_file]\n",
    "print(group)\n",
    "sorted_group = sorted(group, key=lambda x: int(\n",
    "    re.findall(r'.*s(\\d+).*', x)[0]))[:20]\n",
    "\n",
    "all_c2c_sim_list = []\n",
    "all_c2i_sim_list = []\n",
    "c2i_sim_list = []\n",
    "c2c_sim_list = []\n",
    "for _file in sorted_group:\n",
    "    pipe_copy = copy.deepcopy(pipe).to(device)\n",
    "    lora_path = os.path.join(target_dir, _file)\n",
    "    print(f\"loading from: {lora_path}\")\n",
    "    patch_pipe(\n",
    "        pipe_copy,\n",
    "        lora_path,\n",
    "        patch_text=False,\n",
    "        patch_ti=True,\n",
    "        patch_unet=True,\n",
    "        filter_crossattn_str='cross'\n",
    "    )\n",
    "\n",
    "    # processor.tokenizer = pipe_copy.tokenizer\n",
    "    # model.text_model = copy.deepcopy(\n",
    "    #     pipe_copy.text_encoder.text_model.to(device, dtype=weight_dtype))\n",
    "    # # token_embedding = model.text_model.embeddings.token_embedding\n",
    "    # # token_embedding.weight.data[49408] = token_embedding.weight.data[42170]\n",
    "    # inputs = processor(text=captions, images=images,\n",
    "    #                    return_tensors=\"pt\", padding=True).to(device)\n",
    "    # outputs = model(**inputs)\n",
    "\n",
    "    # image_embeds = outputs.image_embeds\n",
    "    # input_ids = inputs.input_ids\n",
    "    # last_hidden_state = outputs.text_model_output.last_hidden_state\n",
    "    # text_embeds = last_hidden_state[torch.arange(last_hidden_state.size(0), device=input_ids.device),\n",
    "    #                                 [(row == 49407).nonzero().min() for row in input_ids]]\n",
    "    # text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "    # logit_scale = model.logit_scale.exp()\n",
    "    # logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n",
    "    # logits_per_image = logits_per_text.t()\n",
    "    # c2c_sim = torch.matmul(text_embeds, text_embeds.t())\n",
    "    # c2c_sim_list.append(c2c_sim.detach().cpu().numpy())\n",
    "    # # we can take the softmax to get the label probabilities\n",
    "    # probs = logits_per_image.softmax(dim=1)\n",
    "    # c2i_sim_list.append(probs[:, 0].mean().item())\n",
    "\n",
    "all_c2c_sim_list.append(c2c_sim_list)\n",
    "all_c2i_sim_list.append(c2i_sim_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text和 Text 之间的 similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from itertools import product\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from custom_datasets.utils import LIVE_SUBJECT_PROMPT_LIST\n",
    "\n",
    "target_dir = '/data/zhicai/code/Text-regularized-customization/outputs/checkpoints/dog/TiReg/TiReg_norm0.37_weight_0_Init_rank_10_seed_0'\n",
    "ckpt_pattern = r'lora_weight_s\\d{3,4}.safetensors'\n",
    "# captions = [\"photo of a <krk1> dog swimming in a pool\", \"photo of a <krk1> dog \",  \"photo of a dog swimming in a pool\"]\n",
    "weight_dtype = torch.float32\n",
    "caption_lists = [\n",
    "    [template.format('<krk1> dog'),\n",
    "     \"photo of a <krk1> dog\",\n",
    "     template.format('dog')]\n",
    "    for template in LIVE_SUBJECT_PROMPT_LIST\n",
    "]\n",
    "model = CLIPModel.from_pretrained(\n",
    "    \"models/clip-vit-large-patch14\",local_files_only=True).to(device, dtype=weight_dtype)\n",
    "\n",
    "\n",
    "group = [ckpt_file for ckpt_file in os.listdir(\n",
    "    target_dir) if '_s' in ckpt_file and 'lora_weight' in ckpt_file]\n",
    "print(group)\n",
    "sorted_group = sorted(group, key=lambda x: int(\n",
    "    re.findall(r'.*s(\\d+).*', x)[0]))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2c_sim_outer_list = []\n",
    "c2c_sim_iner_list = []\n",
    "for _file in sorted_group:\n",
    "    pipe_copy = copy.deepcopy(pipe).to(device)\n",
    "    lora_path = os.path.join(target_dir, _file)\n",
    "    print(f\"loading from: {lora_path}\")\n",
    "    patch_pipe(\n",
    "        pipe_copy,\n",
    "        lora_path,\n",
    "        patch_text=False,\n",
    "        patch_ti=True,\n",
    "        patch_unet=True,\n",
    "        filter_crossattn_str='cross'\n",
    "    )\n",
    "\n",
    "    model.text_model = copy.deepcopy(\n",
    "        pipe_copy.text_encoder.text_model.to(device, dtype=weight_dtype))\n",
    "    # token_embedding = model.text_model.embeddings.token_embedding\n",
    "    # token_embedding.weight.data[49408] = token_embedding.weight.data[42170]\n",
    "    c2c_sim_iner_list = []\n",
    "    for captions in caption_lists:\n",
    "        input_ids = pipe_copy.tokenizer(\n",
    "            captions, return_tensors=\"pt\", padding=True).input_ids.to(device)\n",
    "        outputs = pipe_copy.text_encoder.text_model(input_ids)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        pooled_output = last_hidden_state[torch.arange(last_hidden_state.size(0), device=input_ids.device),\n",
    "                                          [(row == 49407).nonzero().min() for row in input_ids]]\n",
    "        text_embeds = model.text_projection(pooled_output)\n",
    "        text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "        logit_scale = model.logit_scale.exp()\n",
    "        c2c_sim = torch.matmul(text_embeds, text_embeds.t())\n",
    "        diag_idx = torch.arange(min(c2c_sim.size(0), c2c_sim.size(1)))\n",
    "        c2c_sim = c2c_sim[0, 1:]\n",
    "        c2c_sim = torch.softmax(c2c_sim, dim=-1)\n",
    "        c2c_sim_iner_list.append(c2c_sim.detach().cpu().numpy()[0])\n",
    "    c2c_sim_outer_list.append(np.array(c2c_sim_iner_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MovAvg(object):\n",
    "    def __init__(self, window_size=7):\n",
    "        self.window_size = window_size\n",
    "        self.data_queue = []\n",
    "\n",
    "    def update(self, data):\n",
    "        if len(self.data_queue) == self.window_size:\n",
    "            del self.data_queue[0]\n",
    "        self.data_queue.append(data)\n",
    "        return sum(self.data_queue)/len(self.data_queue)\n",
    "\n",
    "\n",
    "font = {'family': 'serif',\n",
    "        'weight': 'normal',\n",
    "        'size': 12,\n",
    "        }\n",
    "\n",
    "data = np.stack(c2c_sim_outer_list)\n",
    "\n",
    "# 计算每列数据的均值和标准差\n",
    "mean_values = np.mean(data, axis=1)\n",
    "smooth_fn = MovAvg(2)\n",
    "smoothed_mean_values = [smooth_fn.update(value) for value in mean_values]\n",
    "std_values = np.std(data, axis=1)\n",
    "\n",
    "x_labels = list(range(0, 100 * len(data), 100))\n",
    "\n",
    "ax = plt.errorbar(x=x_labels, y=smoothed_mean_values, yerr=0.2 * std_values, c='black', capsize=0.4, marker='s',\n",
    "                  ecolor='#79AC78', mec='#79AC78', mfc='#79AC78', ms=3, mew=4, alpha=0.7)\n",
    "\n",
    "# plt.title()\n",
    "plt.xticks(list(range(0, 100 * len(data) + 100, 200)))\n",
    "plt.xlabel('steps', fontdict=font)\n",
    "plt.ylabel('similarity', fontdict=font)\n",
    "plt.grid(axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.clip import CLIPTextModel\n",
    "\n",
    "lora_path = '/data/zhicai/code/Text-regularized-customization/outputs/checkpoints/dog/TiReg/TiReg_weight_0_rank_10_seed_0/lora_weight_s2000.safetensors'\n",
    "pipe_copy = copy.deepcopy(pipe).to(device)\n",
    "patch_pipe(\n",
    "    pipe_copy,\n",
    "    lora_path,\n",
    "    patch_text=False,\n",
    "    patch_ti=True,\n",
    "    patch_unet=True,\n",
    "    filter_crossattn_str='cross'\n",
    ")\n",
    "\n",
    "prompt = [\"a photo of a ktn dog swimming in a pool\",\n",
    "          \"a photo of a <krk1> dog swimming in a pool\"]\n",
    "text_inputs = pipe_copy.tokenizer(\n",
    "    prompt,\n",
    "    padding=\"max_length\",\n",
    "    max_length=pipe.tokenizer.model_max_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "device = pipe_copy.text_encoder.device\n",
    "text_input_ids = text_inputs.input_ids.to(device)\n",
    "global_eot_ids = [(row == 49407).nonzero().min() for row in text_input_ids]\n",
    "bs = len(prompt)\n",
    "outputs = pipe_copy.text_encoder(\n",
    "    text_input_ids.to(device),\n",
    "    attention_mask=None,\n",
    "    output_attentions=True,\n",
    ")\n",
    "attentions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# data = sns.load_dataset(\"flights\")\n",
    "# data = data.pivot(\"month\", \"year\", \"passengers\")\n",
    "attentions = []\n",
    "for attention in outputs.attentions:\n",
    "    attentions.append(attention.mean(dim=1)[torch.arange(\n",
    "        bs, device=device), 11, :12].detach().cpu().numpy())\n",
    "attentions = np.stack(attentions, axis=1)\n",
    "idx = 1\n",
    "data1 = attentions[idx].round(2)\n",
    "xticklabels = ['<sot>'] + prompt[idx].split(' ') + ['<eot>']\n",
    "yticklabels = np.arange(12) + 1\n",
    "sns.set(font_scale=0.7)\n",
    "ax = sns.heatmap(data1, annot=True, xticklabels=xticklabels,\n",
    "                 yticklabels=yticklabels, linewidths=.5, cmap=\"YlGnBu\")\n",
    "ax.set_xlabel(\"prompt\")\n",
    "ax.set_ylabel(\"layer\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# data = sns.load_dataset(\"flights\")\n",
    "# data = data.pivot(\"month\", \"year\", \"passengers\")\n",
    "prompt = ['a photo of a <new> dog swimming in a pool']\n",
    "attentions = []\n",
    "for attention in outputs.attentions:\n",
    "    attentions.append(attention.mean(dim=1)[torch.arange(\n",
    "        bs, device=device), 11, :12].detach().cpu().numpy())\n",
    "attentions = np.stack(attentions, axis=1)\n",
    "idx = 0\n",
    "data2 = attentions[1].round(2) - attentions[0].round(2)\n",
    "xticklabels = ['<sot>'] + prompt[idx].split(' ') + ['<eot>']\n",
    "yticklabels = np.arange(12) + 1\n",
    "sns.set(font_scale=0.7)\n",
    "ax = sns.heatmap(data2, annot=True, xticklabels=xticklabels,\n",
    "                 yticklabels=yticklabels, linewidths=.5, cmap=\"YlGnBu\")\n",
    "ax.set_xlabel(\"token\")\n",
    "ax.set_ylabel(\"text encoder layer\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计Lora中的self和cross attn模块中的Linear-layer权重漂移和对语义漂移"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lora_diffusion.lora import filter_unet_to_norm_weights\n",
    "import copy\n",
    "target_dir = '/data/zhicai/code/Text-regularized-customization/outputs/checkpoints/dog/TiReg/TiReg_weight_0_rank_10_seed_0'\n",
    "test_prompts = ['a dog', 'a cat', 'a teddybear', 'a chair']\n",
    "reg_prompt = ['photo of a ktn dog swimming in a pool']\n",
    "custom_prompt = ['photo of a <krk1> dog swimming in a pool']\n",
    "\n",
    "k_projected_reg_conditions = []\n",
    "k_projected_input_conditions = []\n",
    "\n",
    "# 找到cross_attn 和 self_attn的位置\n",
    "cnt = 0\n",
    "\n",
    "\n",
    "group = [ckpt_file for ckpt_file in os.listdir(\n",
    "    target_dir) if '_s' in ckpt_file and 'lora_weight' in ckpt_file]\n",
    "print(group)\n",
    "sorted_group = sorted(group, key=lambda x: int(\n",
    "    re.findall(r'.*s(\\d+).*', x)[0]))[:20]\n",
    "\n",
    "pipe_copy = copy.deepcopy(pipe).to(device)\n",
    "for ckpt_file in sorted_group:\n",
    "    cur_step_list_cross = []\n",
    "    cur_step_context_reg_reside = []\n",
    "    cur_step_list_self = []\n",
    "    lora_ckpt = os.path.join(target_dir, ckpt_file)\n",
    "    patch_pipe(\n",
    "        pipe_copy,\n",
    "        lora_ckpt,\n",
    "        patch_text=False,\n",
    "        patch_ti=True,\n",
    "        patch_unet=True,\n",
    "        filter_crossattn_str='cross'\n",
    "    )\n",
    "    tune_lora_scale(pipe_copy.unet, 0)\n",
    "    reg_conditions = pipe_copy._encode_prompt(reg_prompt, device, 1, False)\n",
    "    input_conditions = pipe_copy._encode_prompt(\n",
    "        custom_prompt, device, 1, False)\n",
    "    to_reg_params = filter_unet_to_norm_weights(\n",
    "        pipe_copy.unet, target_replace_module=UNET_CROSSATTN_TARGET_REPLACE)\n",
    "    layer_projected_input_conditions = []\n",
    "    layer_projected_reg_conditions = []\n",
    "    for module in to_reg_params[\"cross_project_k_loras\"]:\n",
    "        layer_projected_input_conditions.append(\n",
    "            module.linear(input_conditions).norm(dim=-1))\n",
    "        layer_projected_reg_conditions.append(\n",
    "            module.linear(reg_conditions).norm(dim=-1))\n",
    "    k_projected_reg_conditions.append(\n",
    "        torch.stack(layer_projected_reg_conditions))\n",
    "    k_projected_input_conditions.append(\n",
    "        torch.stack(layer_projected_input_conditions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpts, layers, 77\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig1 = k_projected_reg_conditions[-1].squeeze().detach().cpu().numpy().round(3)\n",
    "fig2 = k_projected_input_conditions[-1].squeeze(\n",
    ").detach().cpu().numpy().round(3)\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.set(font_scale=0.7)\n",
    "\n",
    "ax = sns.heatmap(fig2, annot=False, linewidths=.5, cmap=\"YlGnBu\")\n",
    "ax.set_xlabel(\"layers\")\n",
    "ax.set_ylabel(\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig2 = k_projected_input_conditions[0].squeeze(\n",
    ").detach().cpu().numpy().round(3)\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.set(font_scale=0.7)\n",
    "\n",
    "ax = sns.heatmap(fig2, annot=False, linewidths=.5, cmap=\"YlGnBu\")\n",
    "ax.set_xlabel(\"layers\")\n",
    "ax.set_ylabel(\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reg_lora.vis_attn_map import add_attn_vis_hook, get_attn_softmax\n",
    "name = 'up_blocks.2.attentions.1.transformer_blocks.0.attn2'\n",
    "prompt = 'photo of a <krk1> dog swimming in a pool'\n",
    "activation = {}\n",
    "hooks = []\n",
    "add_attn_vis_hook(pipe_copy.unet, name)\n",
    "images = pipe_copy([prompt], num_inference_steps=50, guidance_scale=6).images\n",
    "del pipe_copy\n",
    "visualize_images(images,  nrow=1, outpath=path +\n",
    "                 '_ori_img', show=True, save=False,)\n",
    "\n",
    "attn_map = activation[name]\n",
    "for i in range(len(attn_map)):\n",
    "    #  attn_map is of shape [8+8 , 4096, 77]\n",
    "    # then 40th sampling step\n",
    "    if i == 40:\n",
    "        fig_shape = int(math.sqrt(attn_map[i].size(1)))\n",
    "        # reshape to (shape,shape) and average over heads\n",
    "        vis_map = attn_map[i].reshape(\n",
    "            attn_map[i].size(0), fig_shape, fig_shape, 77)\n",
    "        uncond_attn_map, cond_attn_map = torch.chunk(vis_map, 2, dim=0)\n",
    "\n",
    "        # mean over head [h, w, 77]\n",
    "        cond_attn_map = cond_attn_map.mean(0)\n",
    "        uncond_attn_map = uncond_attn_map.mean(0)\n",
    "\n",
    "        fig, ax = plt.subplots(1, 10, figsize=(20, 2))\n",
    "        for j in range(10):\n",
    "            map = cond_attn_map[:, :, j].unsqueeze(-1).cpu().numpy()\n",
    "            ax[j].imshow(map)\n",
    "            # no axis for subplot\n",
    "            ax[j].axis('off')\n",
    "        # plt.savefig(f'{path}_attn_map.jpg')\n",
    "        # plt.imshow(vis_map[:,:,2].cpu().numpy())\n",
    "        # plt.close(fig)\n",
    "        # print(\"saved attn map at \", f\"{path}_attn_map.jpg\")\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前置函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from functools import partial\n",
    "import sys\n",
    "import PIL.Image as Image\n",
    "\n",
    "\n",
    "def patch_and_sample_lora_ckpts(pipe,\n",
    "                                lora_ckpts,\n",
    "                                prompts,\n",
    "                                seed=0,\n",
    "                                unet_scale=0,\n",
    "                                bs=4,\n",
    "                                filter_crossattn_str='cross',\n",
    "                                num_inference_steps=50,\n",
    "                                weight_dtype=torch.float16,):\n",
    "    torch.manual_seed(seed)\n",
    "    latents = torch.randn((bs, 4, 64, 64), device=device, dtype=weight_dtype)\n",
    "    # pipe_copy.text_encoder = my_text_encoder\n",
    "    img_list = []\n",
    "    for lora_ckpt in lora_ckpts:\n",
    "        pipe_copy = copy.deepcopy(pipe).to(device)\n",
    "        torch.manual_seed(seed)\n",
    "        patch_pipe(\n",
    "            pipe_copy,\n",
    "            lora_ckpt,\n",
    "            patch_text=False,\n",
    "            patch_ti=True,\n",
    "            patch_unet=True,\n",
    "            filter_crossattn_str=filter_crossattn_str\n",
    "        )\n",
    "        # pipe.unet\n",
    "        tune_lora_scale(pipe_copy.unet, unet_scale)\n",
    "\n",
    "        pipe_copy.text_encoder.text_model.to(device, dtype=weight_dtype)\n",
    "        # tune_lora_scale(pipe_copy.text_encoder, 1)\n",
    "        for prompt in prompts:\n",
    "            prompt = [prompt]*bs\n",
    "            img = pipe_copy(prompt=prompt,\n",
    "                            num_inference_steps=num_inference_steps,\n",
    "                            guidance_scale=6,\n",
    "                            latents=latents,\n",
    "                            callback=None,\n",
    "                            callback_steps=1).images\n",
    "            img_list.extend(img)\n",
    "        del pipe_copy\n",
    "    return img_list\n",
    "\n",
    "\n",
    "def patch_and_translation_lora_ckpts(\n",
    "        translation_pipe: StableDiffusionImg2ImgPipeline,\n",
    "        lora_ckpts,\n",
    "        prompts,\n",
    "        source_img: PIL.Image,\n",
    "        strength=0.5,\n",
    "        seed=0,\n",
    "        unet_scale=0,\n",
    "        bs=4,\n",
    "        filter_crossattn_str='cross',\n",
    "        num_inference_steps=50,\n",
    "        weight_dtype=torch.float16,):\n",
    "    # pipe_copy.text_encoder = my_text_encoder\n",
    "    img_list = []\n",
    "    for lora_ckpt in lora_ckpts:\n",
    "        translation_pipe_copy = copy.deepcopy(translation_pipe).to(device)\n",
    "        torch.manual_seed(seed)\n",
    "        patch_pipe(\n",
    "            translation_pipe_copy,\n",
    "            lora_ckpt,\n",
    "            patch_text=False,\n",
    "            patch_ti=True,\n",
    "            patch_unet=True,\n",
    "            filter_crossattn_str=filter_crossattn_str\n",
    "        )\n",
    "        # pipe.unet\n",
    "        tune_lora_scale(translation_pipe_copy.unet, unet_scale)\n",
    "\n",
    "        translation_pipe_copy.text_encoder.text_model.to(\n",
    "            device, dtype=weight_dtype)\n",
    "        # tune_lora_scale(pipe_copy.text_encoder, 1)\n",
    "        for prompt in prompts:\n",
    "            prompt = [prompt]*bs\n",
    "            img = translation_pipe_copy(\n",
    "                image=source_img,\n",
    "                prompt=prompt,\n",
    "                strength=strength,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                guidance_scale=6,\n",
    "                callback=None,\n",
    "                callback_steps=1).images\n",
    "            img_list.extend(img)\n",
    "        del translation_pipe_copy\n",
    "    return img_list\n",
    "\n",
    "\n",
    "def identity_scale_ratio(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "\n",
    "def linear_scale_ratio(i, t, latents, self, num_inference_steps=50):\n",
    "    scale = np.linspace(0, 1, num_inference_steps)[i]\n",
    "    print(f\"Linear scale ratio: {scale}\")\n",
    "    tune_lora_scale(self.unet, scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "v1_lora_ckpts = [\n",
    "    #   '/data/zhicai/code/Text-regularized-customization/outputs/checkpoints/dog/TiReg/TiReg_weight_0_rank_10_seed_0/lora_weight_s4000.safetensors',\n",
    "    '/data/zhicai/code/Text-regularized-customization/outputs/checkpoints/dog/TiReg/TiReg_norm0.37_weight_0_Init_rank_10_seed_0/lora_weight_s1000.safetensors',\n",
    "    # '/data/zhicai/code/Text-regularized-customization/outputs/checkpoints/dog/TiReg/TiReg_norm0.37_weight_0_Init_rank_10_seed_0/lora_weight_s1000.safetensors',\n",
    "]\n",
    "v1_prompts = ['a photo of a <krk1>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "img_list = patch_and_sample_lora_ckpts(v1_lora_ckpts, v1_prompts)\n",
    "img = visualize_images(img_list, outpath='ewc_shareTI_weight-1.pdf',\n",
    "                       nrow=bs*len(v1_prompts), show=True, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "souce_img = Image.open(\n",
    "    '/data/zhicai/code/Text-regularized-customization/custom_datasets/data/dog7/02.jpg')\n",
    "souce_img = souce_img.resize((512, 512))\n",
    "img_list = patch_and_translation_lora_ckpts(\n",
    "    translation_pipe, v1_lora_ckpts, v1_prompts, souce_img, strength=0.9)\n",
    "img = visualize_images(img_list, outpath='ewc_shareTI_weight-1.pdf',\n",
    "                       nrow=4*len(v1_prompts), show=True, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V* dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from functools import partial\n",
    "import sys\n",
    "\n",
    "v2_lora_ckpts = [\n",
    "\n",
    "    '/data/zhicai/code/Text-regularized-customization/logs_zhicai/cat/2023-10-20T14-12-52_cat_textReg/lora_weight_s100.safetensors',\n",
    "    '/data/zhicai/code/Text-regularized-customization/logs_zhicai/cat/2023-10-20T14-12-52_cat_textReg/lora_weight_s500.safetensors',\n",
    "    '/data/zhicai/code/Text-regularized-customization/logs_zhicai/cat/2023-10-20T14-12-52_cat_textReg/lora_weight_s1000.safetensors',\n",
    "]\n",
    "\n",
    "v2_prompts = ['a photo of a <krk1> cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = patch_and_sample_lora_ckpts(pipe, v2_lora_ckpts, v2_prompts)\n",
    "img = visualize_images(img_list, outpath='ewc_shareTI_weight-1.pdf',\n",
    "                       nrow=4*len(v2_prompts), show=True, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "souce_img = Image.open(\n",
    "    '/data/zhicai/code/Text-regularized-customization/custom_datasets/data/dog7/02.jpg')\n",
    "souce_img = souce_img.resize((512, 512))\n",
    "img_list = patch_and_translation_lora_ckpts(\n",
    "    translation_pipe, v2_lora_ckpts, v2_prompts, souce_img, strength=0.9)\n",
    "img = visualize_images(img_list, outpath='ewc_shareTI_weight-1.pdf',\n",
    "                       nrow=4*len(v2_prompts), show=True, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pipe.text_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4\n",
    "prompt = ['photo of a Artic Tern on road']*bs\n",
    "img_list = pipe(\n",
    "    prompt=prompt,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=6,\n",
    "    callback=None,\n",
    "    callback_steps=1).images\n",
    "img = visualize_images(img_list, outpath='ewc_shareTI_weight-1.pdf',\n",
    "                       nrow=bs, show=True, save=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked attention projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reg_lora.modules import CLIPTiTextModel\n",
    "text_encoder = CLIPTiTextModel.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    subfolder=\"text_encoder\",\n",
    "    revision=None,\n",
    "    mask_identifier_causal_attention=True,\n",
    "    class_token_len=1,\n",
    "    placeholder_token_id=49408,  # We only support on placeholder token for now\n",
    "    local_files_only=True,\n",
    ").to(device, dtype=weight_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from functools import partial\n",
    "import sys\n",
    "\n",
    "lora_ckpts = [\n",
    "    '../outputs/checkpoints/dog/TiReg/TiReg_norm0.37_MaskPlaceholder_weight_0_rank_10_seed_0/lora_weight_s100.safetensors',\n",
    "    '../outputs/checkpoints/dog/TiReg/TiReg_norm0.37_MaskPlaceholder_weight_0_rank_10_seed_0/lora_weight_s500.safetensors',\n",
    "    '../outputs/checkpoints/dog/TiReg/TiReg_norm0.37_MaskPlaceholder_weight_0_rank_10_seed_0/lora_weight_s1000.safetensors',\n",
    "    '../outputs/checkpoints/dog/TiReg/TiReg_norm0.37_MaskPlaceholder_weight_0_rank_10_seed_0/lora_weight_s2000.safetensors',\n",
    "]\n",
    "\n",
    "prompts = ['a photo of a <krk1> dog swimming in a pool']\n",
    "bs = 4\n",
    "num_inference_steps = 50\n",
    "weight_dtype = torch.float16\n",
    "use_scale = False\n",
    "\n",
    "latents = torch.randn((bs, 4, 64, 64), device=device, dtype=weight_dtype)\n",
    "# pipe_copy.text_encoder = my_text_encoder\n",
    "img_list = []\n",
    "for lora_ckpt in lora_ckpts:\n",
    "    pipe_copy = copy.deepcopy(pipe).to(device)\n",
    "    pipe_copy.text_encoder = text_encoder\n",
    "    torch.manual_seed(2)\n",
    "    patch_pipe(\n",
    "        pipe_copy,\n",
    "        lora_ckpt,\n",
    "        patch_text=False,\n",
    "        patch_ti=True,\n",
    "        patch_unet=True,\n",
    "        filter_crossattn_str='cross'\n",
    "    )\n",
    "    # pipe.unet\n",
    "    tune_lora_scale(pipe_copy.unet, 0)\n",
    "\n",
    "    pipe_copy.text_encoder.text_model.to(device, dtype=weight_dtype)\n",
    "    # tune_lora_scale(pipe_copy.text_encoder, 1)\n",
    "    for prompt in prompts:\n",
    "        prompt = [prompt]*bs\n",
    "        img = pipe_copy(prompt=prompt,\n",
    "                        num_inference_steps=num_inference_steps,\n",
    "                        guidance_scale=6,\n",
    "                        latents=latents,\n",
    "                        callback=None,\n",
    "                        callback_steps=1).images\n",
    "        img_list.extend(img)\n",
    "img = visualize_images(img_list, outpath='ewc_shareTI_weight-1.pdf',\n",
    "                       nrow=bs*len(prompts), show=True, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "path_list = glob.glob(\n",
    "    '/data/zhicai/code/Text-regularized-customization/outputs/checkpoints/wooden_pot/ewc_reg_shareTI/ewc_gen_weight_0.01_rank_10_seed_*/fisher.pt')\n",
    "path_list.extend(glob.glob(\n",
    "    '/data/zhicai/code/Text-regularized-customization/outputs/checkpoints/wooden_pot/ewc_reg_shareTI/ewc_real_weight_0.01_rank_10_seed_*/fisher.pt'))\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 4))\n",
    "for path in path_list:\n",
    "    ewc_statics = torch.load(path)\n",
    "    fishers = ewc_statics['fisher_list']\n",
    "    lora_ups = ewc_statics['lora_up_list']\n",
    "    lora_downs = ewc_statics['lora_down_list']\n",
    "    fisher_mean = [fisher.pow(2).cpu().numpy().mean() for fisher in fishers]\n",
    "    lora_down_mean = [lora_down.pow(2).cpu().numpy().mean()\n",
    "                      for lora_down in lora_downs]\n",
    "    axes[0].plot(fisher_mean)\n",
    "    axes[1].plot(lora_down_mean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lora_diffusion.lora import filter_unet_to_norm_weights\n",
    "from safetensors.torch import safe_open\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "lora_ckpt = '../outputs/checkpoints/dog/ewc_reg_shareTI/freeze-down_lora_rank_10/lora_weight_s1100.safetensors'\n",
    "patch_pipe(\n",
    "    pipe,\n",
    "    lora_ckpt,\n",
    "    patch_text=False,\n",
    "    patch_ti=True,\n",
    "    patch_unet=True,\n",
    "    filter_crossattn_str='cross+self'\n",
    ")\n",
    "to_reg_params = filter_unet_to_norm_weights(\n",
    "    pipe.unet, target_replace_module=UNET_CROSSATTN_TARGET_REPLACE)\n",
    "lora_up_list = []\n",
    "lora_down_list = []\n",
    "for module in to_reg_params[\"other_loras\"]:\n",
    "    lora_up_list.append(module.lora_up.weight.data.norm().cpu())\n",
    "    lora_down_list.append(module.lora_down.weight.data.mean().cpu())\n",
    "plt.plot(lora_up_list, label='lora_up')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,10))\n",
    "fig, axs = plt.subplots(8, 4, figsize=(20, 10))\n",
    "for i in range(len(fishers)):\n",
    "    fisher = fishers[i].cpu().numpy().flatten()\n",
    "    mean = fisher.mean()\n",
    "    fisher = fisher/mean\n",
    "    axs[i//4, i % 4].hist(fisher, bins=100)\n",
    "    axs[i//4, i % 4].tick_params(labelsize=5)\n",
    "    # plt.hist(fishers[i].cpu().numpy().flatten(), bins=100)\n",
    "    # plt.show()\n",
    "plt.savefig('fisher_distribution.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fisher in fishers:\n",
    "    fisher = fisher.cpu().numpy()\n",
    "    mean = fisher.mean()\n",
    "    fisher = fisher/mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.randn(3, 3).pow(2)/100\n",
    "det_a = torch.det(a)\n",
    "norm_a = a.norm(2)\n",
    "a_norm = a / norm_a\n",
    "norm_a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
