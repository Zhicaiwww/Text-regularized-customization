{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial : Running minimal inference examples with diffuser.\n",
    "\n",
    "For this tutorial, we will use my pre-trained lora embedding that is pretrained on pop-arts, illustrations and pixar footages.\n",
    "\n",
    "To get started install this package with:\n",
    "\n",
    "```bash\n",
    "pip install git+https://github.com/cloneofsimo/lora.git\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/zhicai/miniconda3/envs/ldm/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-05-27 08:35:30.987606: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9252d476d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline, EulerAncestralDiscreteScheduler\n",
    "import torch\n",
    "import os\n",
    "# os.environ[\"DISABLE_TELEMETRY\"] = 'YES'\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16,local_files_only=True,revision='39593d5650112b4cc580433f6b0435385882d819').to(\n",
    "    \"cuda:4\"\n",
    ")\n",
    "pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "prompt = \"style of <s1><s2>, baby lion\"\n",
    "torch.manual_seed(0)\n",
    "# image = pipe(prompt, num_inference_steps=50, guidance_scale=7).images[0]\n",
    "\n",
    "# image  # nice. diffusers are cool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! load_ext autoreload\n",
    "! autoreload 2\n",
    "import json\n",
    "import math\n",
    "from itertools import groupby\n",
    "from typing import Callable, Dict, List, Optional, Set, Tuple, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from lora_diffusion import LoraInjectedConv2d, LoraInjectedLinear, patch_pipe, tune_lora_scale\n",
    "from lora_diffusion.lora import _find_modules, UNET_CROSSATTN_TARGET_REPLACE\n",
    "from visual import visualize_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lora_diffusion import LoraInjectedLinear\n",
    "layer = LoraInjectedLinear(10,20)\n",
    "layer.get_reg_loss(reg_vector=torch.ones(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change a PIL.Image to  tensor\n",
    "def get_masked_identifier_latents(text_encoder,token_ids,identifier_indice,class_len,dtype=torch.float16,replace_identifier = True):\n",
    "    hidden_states = text_encoder.text_model.embeddings(input_ids=token_ids.to(text_encoder.device))\n",
    "    bs = token_ids.size(0)\n",
    "    identifier_indice = identifier_indice\n",
    "    class_token_len = class_len\n",
    "    causal_attention_mask = text_encoder.text_model._build_causal_attention_mask(bs,77,dtype)\n",
    "    causal_attention_mask[:,:,identifier_indice, :max(identifier_indice,1)] = torch.finfo(dtype).min\n",
    "    causal_attention_mask[:,:,identifier_indice+class_token_len+1:,identifier_indice] = torch.finfo(dtype).min\n",
    "    encoder_outputs = text_encoder.text_model.encoder(\n",
    "    inputs_embeds=hidden_states,\n",
    "    causal_attention_mask=causal_attention_mask.to(text_encoder.device),\n",
    "    )\n",
    "\n",
    "    last_hidden_state = encoder_outputs[0]\n",
    "    encoder_hidden_states = text_encoder.text_model.final_layer_norm(last_hidden_state)\n",
    "    # encoder_hidden_states[:,identifier_indice] = 0.5 * encoder_hidden_states[:,identifier_indice]\n",
    "    return encoder_hidden_states\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs  = 2\n",
    "prompt = ['a dog in the beach'] * bs\n",
    "tokens = pipe.tokenizer(prompt,return_tensors='pt',padding='max_length',truncation=False,max_length=77)\n",
    "embed = pipe.text_encoder(tokens['input_ids'].to(pipe.text_encoder.device))[0]\n",
    "no_mask_images = pipe(prompt_embeds= embed, num_inference_steps=50, guidance_scale=7).images\n",
    "embed = get_masked_identifier_latents(pipe.text_encoder,tokens['input_ids'],1,2)\n",
    "mask_images = pipe(prompt_embeds= embed, num_inference_steps=50, guidance_scale=7).images\n",
    "visualize_images(no_mask_images+mask_images,prompt=prompt[0],outpath='figure/',nrow=2,type = 'mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "import copy\n",
    "lora_ckpts = ['../output_dog_crossOnly_tR0.01/lora_weight_e12_s2000.safetensors']\n",
    "prompts = ['photo of a dog','photo of a <krk> dog', 'a <krk> dog at a beach with a view of the seashore']\n",
    "bs = 8\n",
    "pipe_copy = copy.deepcopy(pipe)\n",
    "torch.manual_seed(0)\n",
    "patch_pipe(\n",
    "    pipe_copy,\n",
    "    lora_ckpts[0],\n",
    "    patch_text=False,\n",
    "    patch_ti=True,\n",
    "    patch_unet=True,\n",
    "    filter_crossattn_str = 'cross'\n",
    ")\n",
    "# pipe.unet\n",
    "tune_lora_scale(pipe_copy.unet, 0)\n",
    "# tune_lora_scale(pipe_copy.text_encoder, 1)\n",
    "for prompt in prompts:\n",
    "    prompt = [prompt]*bs\n",
    "    img = pipe_copy(prompt = prompt, num_inference_steps=50, guidance_scale=6).images\n",
    "    visualize_images(img,outpath='figure/', nrow=4, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "import copy\n",
    "_lora_path = ['../output_dog_cross+self/lora_weight_e12_s2000.safetensors']\n",
    "lora_ckpts = ['../output_dog_cross+self/lora_weight_e12_s2000.safetensors']\n",
    "prompts = ['photo of a dog','photo of a <krk> dog', 'a <krk> dog at a beach with a view of the seashore']\n",
    "bs = 8\n",
    "del pipe_copy\n",
    "pipe_copy = copy.deepcopy(\n",
    "    pipe)\n",
    "torch.manual_seed(0)\n",
    "patch_pipe(\n",
    "    pipe_copy,\n",
    "    lora_ckpts[0],\n",
    "    patch_text=False,\n",
    "    patch_ti=True,\n",
    "    patch_unet=True,\n",
    "    filter_crossattn_str = 'cross+self'\n",
    ")\n",
    "# pipe.unet\n",
    "tune_lora_scale(pipe_copy.unet, 1)\n",
    "# tune_lora_scale(pipe_copy.text_encoder, 1)\n",
    "for prompt in prompts:\n",
    "    prompt = [prompt]*bs\n",
    "    img = pipe_copy(prompt = prompt, num_inference_steps=50, guidance_scale=6).images\n",
    "    visualize_images(img,outpath='figure/', nrow=4, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "import copy\n",
    "lora_ckpts = ['../output_dog_cross+self_tR0.001_nR0.001/lora_weight_e12_s2000.safetensors']\n",
    "prompts = ['photo of a dog','photo of a <krk> dog', 'a <krk> dog at a beach with a view of the seashore']\n",
    "bs = 8\n",
    "pipe_copy = copy.deepcopy(pipe)\n",
    "torch.manual_seed(0)\n",
    "patch_pipe(\n",
    "    pipe_copy,\n",
    "    lora_ckpts[0],\n",
    "    patch_text=False,\n",
    "    patch_ti=True,\n",
    "    patch_unet=True,\n",
    "    filter_crossattn_str = 'cross+self'\n",
    ")\n",
    "# pipe.unet\n",
    "tune_lora_scale(pipe_copy.unet, 1)\n",
    "# tune_lora_scale(pipe_copy.text_encoder, 1)\n",
    "for prompt in prompts:\n",
    "    prompt = [prompt]*bs\n",
    "    img = pipe_copy(prompt = prompt, num_inference_steps=50, guidance_scale=6).images\n",
    "    visualize_images(img,outpath='figure/', nrow=4, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_ckpts = ['../output_dog/lora_weight_e12_s2000.safetensors']\n",
    "prompts = ['photo of a dog']\n",
    "bs = 4\n",
    "torch.manual_seed(0)\n",
    "for lora_ckpt in lora_ckpts: \n",
    "    patch_pipe(\n",
    "        pipe,\n",
    "        lora_ckpt,\n",
    "        patch_text=True,\n",
    "        patch_ti=True,\n",
    "        patch_unet=True,\n",
    "    )\n",
    "    tune_lora_scale(pipe.unet, 0.1)\n",
    "    tune_lora_scale(pipe.text_encoder, 1)\n",
    "    guidance_scale = 7\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for prompt in prompts:\n",
    "            prompt = [prompt] * bs\n",
    "            tokens = pipe.tokenizer(prompt,return_tensors='pt',padding='max_length',truncation=False,max_length=77)\n",
    "            un_tokens = pipe.tokenizer([''] * bs,return_tensors='pt',padding='max_length',truncation=False,max_length=77)\n",
    "            embed = pipe.text_encoder(tokens['input_ids'].to(pipe.text_encoder.device))[0]\n",
    "            un_embed = pipe.text_encoder(un_tokens['input_ids'].to(pipe.text_encoder.device))[0]\n",
    "            c_embed = torch.concat([un_embed,embed],dim=0)\n",
    "            # embed = get_masked_identifier_latents(pipe.text_encoder,tokens['input_ids'],5,1)\n",
    "            pipe.scheduler.set_timesteps(50)\n",
    "\n",
    "            sample_size = pipe.unet.config.sample_size\n",
    "            noise = torch.randn((bs , 4, sample_size, sample_size),dtype=torch.float16).to(\"cuda\")\n",
    "            latents = noise\n",
    "            for t in pipe.scheduler.timesteps:\n",
    "                    latent_model_input = torch.cat([latents] * 2)\n",
    "                    latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)\n",
    "                    noise_pred = pipe.unet(latent_model_input, t,encoder_hidden_states=c_embed).sample\n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "                    prev_noisy_sample = pipe.scheduler.step(noise_pred, t, latents).prev_sample\n",
    "                    latents = prev_noisy_sample\n",
    "            image = pipe.decode_latents(latents)\n",
    "            image = pipe.numpy_to_pil(image)\n",
    "            image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_ckpts = ['../output_dog/lora_weight_e12_s2000.safetensors']\n",
    "prompts = ['photo of a <krk> dog','photo of a <krk> dog swimming in a pool']\n",
    "bs = 2\n",
    "for lora_ckpt in lora_ckpts: \n",
    "    patch_pipe(\n",
    "        pipe,\n",
    "        lora_ckpt,\n",
    "        patch_text=True,\n",
    "        patch_ti=True,\n",
    "        patch_unet=True,\n",
    "    )\n",
    "    for prompt in prompts:\n",
    "        scale_type = 'all'\n",
    "        images = visual_unet_scales(pipe,prompt,type= scale_type,seed=1,batch_size=bs,scales=[1])\n",
    "        outpath = os.path.join(*lora_ckpt.split('/')[:2],'grid_samples')\n",
    "        visualize_images(images,prompt,outpath,nrow=bs, save = False, type = scale_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "lora_ckpts = ['../output_dog_selfOnly/lora_weight_e12_s2000.safetensors']\n",
    "prompts = ['photo of a dog','photo of a <krk> dog', 'a <krk> dog at a beach with a view of the seashore']\n",
    "bs = 8\n",
    "pipe_copy = copy.deepcopy(pipe)\n",
    "torch.manual_seed(0)\n",
    "patch_pipe(\n",
    "    pipe_copy,\n",
    "    lora_ckpts[0],\n",
    "    patch_text=False,\n",
    "    patch_ti=True,\n",
    "    patch_unet=True,\n",
    "    filter_crossattn_str = 'self'\n",
    ")\n",
    "def filter_unet_lora_weights(unet, target_replace_module=UNET_CROSSATTN_TARGET_REPLACE):\n",
    "    \"\"\"\n",
    "    filter out lora weights from unet\n",
    "    returns :\n",
    "        {\"cross_project_loras\": lora_params_name,\n",
    "        \"other_loras\": lora_params_name,\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    _child_modules = [_child_module for _,_,_child_module in \n",
    "                      _find_modules(unet, target_replace_module, search_class=[LoraInjectedLinear, LoraInjectedConv2d])]\n",
    "    _child_cross_project_modules = [_child_module for _,_,_child_module in\n",
    "                                    _find_modules(unet, target_replace_module, search_class=[LoraInjectedLinear],filter_crossattn_str='cross')]\n",
    "\n",
    "    _child_other_lora_modules = list(set(_child_modules) - set(_child_cross_project_modules))\n",
    "\n",
    "    filter_result = {\n",
    "        \"cross_project_loras\": _child_cross_project_modules,\n",
    "        \"other_loras\": _child_other_lora_modules,\n",
    "    }\n",
    "    return filter_result\n",
    "filter_unet_lora_weights(pipe_copy.unet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice. Let's try another example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\n",
    "    \"cuda\"\n",
    ")\n",
    "\n",
    "prompt = \"superman, style of <s1><s2>\"\n",
    "torch.manual_seed(1)\n",
    "image = pipe(prompt, num_inference_steps=50, guidance_scale=4).images[0]\n",
    "\n",
    "image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_pipe(\n",
    "    pipe,\n",
    "    \"../example_loras/lora_popart.safetensors\",\n",
    "    patch_text=True,\n",
    "    patch_ti=True,\n",
    "    patch_unet=True,\n",
    ")\n",
    "torch.manual_seed(1)\n",
    "tune_lora_scale(pipe.unet, 1.0)\n",
    "tune_lora_scale(pipe.text_encoder, 1.0)\n",
    "image = pipe(prompt, num_inference_steps=50, guidance_scale=4).images[0]\n",
    "image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is good pop-art style, but we might get a better result with lower $\\alpha$ for both text encoder and unet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "tune_lora_scale(pipe.unet, 0.5)\n",
    "tune_lora_scale(pipe.text_encoder, 0.5)\n",
    "\n",
    "image = pipe(prompt, num_inference_steps=50, guidance_scale=4.0).images[0]\n",
    "image.save(\"../contents/pop_art.jpg\")\n",
    "image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix : To make stuff on the readme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"baby lion in style of <s1><s2>\"\n",
    "\n",
    "patch_pipe(\n",
    "    pipe,\n",
    "    \"../example_loras/lora_disney.safetensors\",\n",
    "    patch_text=True,\n",
    "    patch_ti=True,\n",
    "    patch_unet=True,\n",
    ")\n",
    "torch.manual_seed(6)\n",
    "tune_lora_scale(pipe.unet, 0.5)\n",
    "tune_lora_scale(pipe.text_encoder, 0.5)\n",
    "image = pipe(prompt, num_inference_steps=50, guidance_scale=5).images[0]\n",
    "image.save(\"../contents/disney_lora.jpg\")\n",
    "image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "patch_pipe(\n",
    "    pipe,\n",
    "    \"../example_loras/lora_krk.safetensors\",\n",
    "    patch_text=True,\n",
    "    patch_ti=True,\n",
    "    patch_unet=True,\n",
    ")\n",
    "\n",
    "example_prompts = [\n",
    "    \"painting of <TOK>, a starry night, style of vincent van gogh\",\n",
    "    \"portrait of <TOK> by mario testino 1950, 1950s style, hair tied in a bun, taken in 1950, detailed face of <TOK>, sony a7r\",\n",
    "    \"photof of <TOK>, 50mm, sharp, muscular, detailed realistic face, hyper realistic, perfect face, intricate, natural light, <TOK> underwater photoshoot,collarbones, skin indentation, Alphonse Mucha, Greg Rutkowski\",\n",
    "    \"a photo of <TOK> in advanced organic armor, biological filigree, detailed symmetric face, flowing hair, neon details, intricate, elegant, highly detailed, digital painting, artstation, concept art, smooth, sharp focus, octane, art by Krenz Cushart , Artem Demura, Alphonse Mucha, digital cgi art 8K HDR by Yuanyuan Wang photorealistic\",\n",
    "    \"a photo of <TOK> on the beach, small waves, detailed symmetric face, beautiful composition\",\n",
    "    \"a photo of <TOK> rainbow background, wlop, dan mumford, artgerm, liam brazier, peter mohrbacher, jia zhangke, 8 k, raw, featured in artstation, octane render, cinematic, elegant, intricate, 8 k\",\n",
    "    \"photo of Summoner <TOK> with a cute water elemental, fantasy illustration, detailed face, intricate, elegant, highly detailed, digital painting, artstation, concept art, wallpaper, smooth, sharp focus, illustration, art by artgerm and greg rutkowski\",\n",
    "    \"<TOK>, cyberpunk 2077, 4K, 3d render in unreal engine\",\n",
    "    \"a pencil sketch of <TOK>\",\n",
    "    \"a minecraft render of <TOK>\",\n",
    "    \"young woman <TOK>, eden, intense eyes, tears running down, crying, vaporwave aesthetic, synthwave, colorful, psychedelic, crown, long gown, flowers, bees, butterflies, ribbons, ornate, intricate, digital painting, artstation, concept art, smooth, sharp focus, illustration of <wday>, art by artgerm and greg rutkowski and alphonse mucha\",\n",
    "    \"<TOK> in a construction outfit\",\n",
    "]\n",
    "\n",
    "outs = []\n",
    "tune_lora_scale(pipe.unet, 0.5)\n",
    "tune_lora_scale(pipe.text_encoder, 0.5)\n",
    "for idx, prompt in enumerate(example_prompts):\n",
    "    prompt = prompt.replace(\"<TOK>\", \"<s1><s2>\")\n",
    "    torch.manual_seed(idx)\n",
    "    image = pipe(prompt, num_inference_steps=50, guidance_scale=6).images[0]\n",
    "    outs.append(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lora_diffusion import image_grid\n",
    "\n",
    "imgs = image_grid(outs, 3, 4)\n",
    "imgs.save(\"../contents/lora_pti_example.jpg\")\n",
    "imgs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using extended LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from lora_diffusion import UNET_EXTENDED_TARGET_REPLACE\n",
    "\n",
    "patch_pipe(\n",
    "    pipe,\n",
    "    \"../example_loras/modern_disney_svd.safetensors\",\n",
    "    patch_text=True,\n",
    "    patch_ti=True,\n",
    "    patch_unet=True,\n",
    "    unet_target_replace_module=UNET_EXTENDED_TARGET_REPLACE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"modern disney style, cute baby lion\"\n",
    "\n",
    "SC = 2.5\n",
    "\n",
    "torch.manual_seed(0)\n",
    "tune_lora_scale(pipe.unet, SC)\n",
    "tune_lora_scale(pipe.text_encoder, SC)\n",
    "\n",
    "img_ori = pipe(\n",
    "    prompt,\n",
    "    num_inference_steps=50,\n",
    "    guidance_scale=7.5,\n",
    "    height=640,\n",
    "    width=512,\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "x = nn.parameter.Parameter(torch.tensor([0.0]),requires_grad=True)\n",
    "optimizer = optim.AdamW([{\"params\": x, \"lr\":0.1},{\"params\": [], \"lr\":0.1}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = x*2\n",
    "b.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "scheduler = DDPMScheduler.from_config(\"runwayml/stable-diffusion-v1-5\",subfolder = \"scheduler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scheduler.alphas_cumprod[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "result = (torch.arange(100) != 1) * (torch.arange(100) != 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch_latest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "952e1bebe1b278d85469a034aefc1854b777c1b518feedf8249123f6f86cec05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
