{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanfid import fid\n",
    "fdir1 = 'retrieve/retrieve_dog/photo_of_a_dog'\n",
    "fdir2 = 'logs/2023-03-26T12-52-38_dog-sdv4/samples'\n",
    "score = fid.compute_kid(fdir1, fdir2)\n",
    "score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute new_concept_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "\n",
    "def to_array(dl):\n",
    "    if isinstance(dl[0],torch.Tensor):\n",
    "        out = [d.cpu().numpy() for d in dl]\n",
    "        return np.stack(out, axis=0)\n",
    "    elif isinstance(dl[0],np.ndarray):\n",
    "        return np.stack(dl, axis=0)\n",
    "    else:\n",
    "        raise NotImplementedError('not supported type')\n",
    "\n",
    "def process_data(path):\n",
    "    out={}\n",
    "    data = torch.load(path)\n",
    "    c_sim = to_array(data.pop('c_sim_global'))\n",
    "    c_new_global =  to_array(data.pop('c_new_global'))\n",
    "    c_reg_global = to_array(data.pop('c_reg_global'))\n",
    "    keys = data.keys()\n",
    "    out['c_sim'] = c_sim\n",
    "    # out['c_sim'] = c_sim\n",
    "    for key in keys:\n",
    "        weights = to_array(data[key])\n",
    "        start_weight =  weights[0]\n",
    "        delta_weights = weights - np.expand_dims(start_weight, axis=0)       \n",
    "        # distance = np.sum(delta_weights,axis=(1,2))\n",
    "        distance = np.linalg.norm(delta_weights,axis=(1,2))\n",
    "        out[key + '_distance'] = distance\n",
    "        # print(distance)\n",
    "        # print(np.transpose(weights[1],(1,0)).shape)\n",
    "        c_mapped_new = [c_n @ np.transpose(weights[i],(1,0)) for i,c_n in zip(range(len(weights)),c_new_global)]\n",
    "        c_mapped_new = np.stack(c_mapped_new, axis=0)\n",
    "        c_mapped_reg = [c_reg_global @ np.transpose(weights[i],(1,0)) for i in range(len(weights))]\n",
    "        c_mapped_reg = np.concatenate(c_mapped_reg, axis=0)\n",
    "        # print(c_mapped_reg.shape)\n",
    "        out[key + '_c_mapped_new'] = c_mapped_new\n",
    "        out[key + '_c_mapped_reg'] = c_mapped_reg\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def draw_line(x1,x2 =None,x3= None,x4=None):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x1, marker='o', linestyle='-', color='r',label = '<new> + prompt')\n",
    "    if x2 is not None:\n",
    "        ax.plot(x2, marker='o', linestyle='--', color='g',label = '<new> + prompt, reg')\n",
    "    if x3 is not None:\n",
    "        ax.plot(x3, marker='s', linestyle='-', color='r',label = '<new>')\n",
    "    if x4 is not None:\n",
    "        ax.plot(x4, marker='s', linestyle='--', color='g',label = '<new>, reg')\n",
    "    plt.legend( loc='upper right',fontsize=7)\n",
    "    plt.xticks(np.arange(0,len(x1) +1),fontsize=7)\n",
    "    plt.show()\n",
    "\n",
    "path = 'logs/2023-03-27T09-11-12_dog_realReg-sdv4/statics.pt'\n",
    "path_reg = 'logs/2023-03-27T09-10-44_dog_genReg-sdv4/statics.pt'\n",
    "out = process_data(path)\n",
    "out_reg = process_data(path_reg)\n",
    "c_mapped_new = []\n",
    "c_mapped_reg = []\n",
    "distance = []\n",
    "for key in out_reg.keys():\n",
    "    if 'distance' in key:\n",
    "        distance.append(key)\n",
    "    elif 'c_mapped_new' in key:\n",
    "        c_mapped_new.append(key)\n",
    "    elif 'c_mapped_reg' in key:\n",
    "        c_mapped_reg.append(key)\n",
    "c_mapped_reg = sorted(c_mapped_reg)\n",
    "c_mapped_new = sorted(c_mapped_new)\n",
    "distance = sorted(distance) \n",
    "\n",
    "\n",
    "N = len(distance)\n",
    "n = 4 # size of each array\n",
    "\n",
    "fig, axs = plt.subplots(N // 4 + (N % 4 != 0), 4,figsize=(10, 8))\n",
    "for i in range(N):\n",
    "    data = out[distance[i]]\n",
    "    data_reg = out_reg[distance[i]]\n",
    "    data_c_reg = out_c_reg[distance[i]]\n",
    "    axs[i // 4][i % 4].plot(data, marker='o', linestyle='-', color='r',linewidth=1,label = 'real reg.')\n",
    "    axs[i // 4][i % 4].plot(data_reg, marker='o', linestyle='-', color='b',linewidth=1,label = 'gen. reg.')\n",
    "    axs[i // 4][i % 4].plot(data_c_reg, marker='o', linestyle='-', color='g',linewidth=1,label = 'gen. reg.')\n",
    "    # plt.xticks(fontsize=3)\n",
    "    # plt.yticks(fontsize=3)\n",
    "plt.savefig('distance.png',dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我给你一个句子，需要你帮助我生成100句语义相近的句子，生成的句子内容可以拓展和补充。以下是一个例子，我给你一句“photo of a dog\", 你可以生成“photo of a corgi on the beach\"，或“A yellow dog rides a skateboard on the street”，如果明白了需求，只需要回复“OK”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,torch\n",
    "import copy\n",
    "path = 'logs/2023-04-03T16-00-34_dogreg_1_scale0.01_a_dog/checkpoints/'\n",
    "ckpts = []\n",
    "for name in os.listdir(path):\n",
    "    path_file = os.path.join(path,name)\n",
    "    ckpt = dict(torch.load(path_file, map_location='cpu')['state_dict'])\n",
    "    keys = ckpt.keys()\n",
    "    for k in list(keys):\n",
    "        if ('attn2.to_k' in k or 'attn2.to_k0' in k or 'attn2.to_v0' in k or 'attn2.to_v' in k):\n",
    "            pass \n",
    "        else:\n",
    "            ckpt.pop(k)\n",
    "    ckpts.append(ckpt)\n",
    "epoch_ckpt = ckpts[-1]\n",
    "\n",
    "delta = epoch_ckpt['model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight'] - epoch_ckpt['model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k0.weight'] \n",
    "flatten_delta2 = delta.flatten()\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax= plt.subplots(figsize=(6,6))\n",
    "ax.hist(flatten_delta,bins=100,label=r\"$\\alpha = 1$,$\\beta$ = 1\")\n",
    "ax.hist(flatten_delta2,bins=100,label=r\"$\\alpha$ = 10,$\\beta$ = 1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = epoch_ckpt['model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight']\n",
    "W0 = torch.load('Stable-diffusion/sd-v1-4-full-ema.ckpt',map_location='cpu')['state_dict']['model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight']\n",
    "delta = W - W0\n",
    "u, s, vt = torch.linalg.svd(delta.clone())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get_deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "for dir in os.listdir('logs'):\n",
    "    path = os.path.join('logs',dir)\n",
    "    os.system(f'python src2/get_deltas.py --path {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src2.custom_modules import FrozenCLIPEmbedderWrapper as CLIP2\n",
    "from src.custom_modules import FrozenCLIPEmbedderWrapper as CLIP1\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "import torch\n",
    "clip1 = CLIP2(\"<new1>\",concept_classes = [\"<new1> tortoise plushy\"],device='cuda')\n",
    "# tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-large-patch14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip1.tokenize('<new1>')\n",
    "new_token = clip1.transformer.get_input_embeddings().weight.data[49408]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src2.custom_modules import FrozenCLIPEmbedderWrapper as CLIP\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "import torch\n",
    "clip = CLIP(\"<new1>\",device='cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute modified similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import re\n",
    "dir = 'logs/2023-04-08T03-32-12_tortoise_plushyreg_0.1_scale0_a_tortoise_plushy_ridge_onlyK_noblip'\n",
    "matchObj = re.match(r'.*(tortoise_plushy|teddybear|cat|dog).*', dir, re.M|re.I)\n",
    "concept = matchObj[1].replace('_',' ')\n",
    "print(concept)\n",
    "sims = []\n",
    "for delta_ckpt in sorted(glob.glob(dir+'/checkpoints/*.ckpt')):\n",
    "    if delta_ckpt is not None:\n",
    "        delta_st = torch.load(delta_ckpt,map_location='cpu')\n",
    "        embed = None\n",
    "        if 'embed' in delta_st['state_dict']:\n",
    "            embed = delta_st['state_dict']['embed'].reshape(-1,768)\n",
    "            del delta_st['state_dict']['embed']\n",
    "        clip.transformer.text_model.embeddings.token_embedding.weight.data[-embed.shape[0]:] = embed\n",
    "    a = clip.encode_text([f'<new1> {concept}'])\n",
    "    b = clip.encode_text([f'{concept}'])\n",
    "    sim = torch.cosine_similarity(a[1],b[1])\n",
    "    sims.append(np.round(sim.item(),4))\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'logit_scale', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'text_projection.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from src2.custom_modules import FrozenCLIPEmbedderWrapper as CLIP\n",
    "clip = CLIP(\"<new1>\",device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = clip.encode(['dog','a dog','red dog','two dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7450, -0.3390, -0.4465,  ...,  0.6784, -0.6734, -0.8623]],\n",
      "\n",
      "        [[-0.7977, -0.0918, -0.6696,  ...,  0.0715, -0.8631, -0.8202]],\n",
      "\n",
      "        [[-1.4391, -1.0661,  0.0901,  ..., -0.0950, -1.0448, -0.0443]],\n",
      "\n",
      "        [[-0.7872, -0.3862,  0.7974,  ...,  0.5418, -1.3921, -0.4970]]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "tensor([[[-0.6378,  0.1839, -1.1026,  ...,  0.5930,  0.0801, -1.0670]],\n",
      "\n",
      "        [[-1.5938,  0.5065,  1.0782,  ..., -1.5271, -0.8438,  0.1604]],\n",
      "\n",
      "        [[-1.6627, -0.0800,  1.6876,  ..., -2.1549, -1.7076,  0.0282]],\n",
      "\n",
      "        [[-0.5260, -0.2593,  1.9496,  ..., -1.3682, -1.4813,  0.2487]]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output[:,20:21,:])\n",
    "print(output[:,2:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ids = []\n",
    "        self.params = {'a':nn.Parameter(torch.randn(1,1,1),requires_grad=True),\n",
    "                        'b':nn.Parameter(torch.randn(1,1,1))}\n",
    "        self.params2 = nn.Parameter(torch.randn(1,1,1),requires_grad=True)\n",
    "        # self.embed = nn.Embedding(10,10)\n",
    "        self.params3 = [nn.Parameter(torch.randn(1,1,1),requires_grad=True)]\n",
    "    def forward(self, x, key):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('params2', tensor([[[0.0913]]]))])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = test()\n",
    "torch.save(net.state_dict(),'test.pt')\n",
    "a = torch.load('test.pt',map_location='cpu')\n",
    "a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPVisionModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, CLIPVisionModel,CLIPProcessor\n",
    "\n",
    "version = \"openai/clip-vit-large-patch14\"\n",
    "model = CLIPVisionModel.from_pretrained(version)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, CLIPVisionModel\n",
    "\n",
    "image = Image.open('data/dog/5.jpeg')\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "pooled_output = outputs.pooler_output  # pooled CLS states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ ,a = clip.encode_text([\"dog\"]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_0_0 = [0.7059, 0.7015, 0.7048, 0.7049, 0.7062]\n",
    "cat_01_0 = [0.6662, 0.6095, 0.5932, 0.5752, 0.5783]\n",
    "cat_10_0 = [0.6705, 0.5937, 0.5625, 0.524, 0.4937]\n",
    "sim_cat = [0.7011, 0.6720]\n",
    "\n",
    "bear_0_0 = [0.7568, 0.7575, 0.757, 0.7559, 0.7567]\n",
    "bear_01_0 = [0.7401, 0.6886, 0.664, 0.6285, 0.6267]\n",
    "bear_01_0 = [0.7377, 0.673, 0.6286, 0.5637, 0.5556]\n",
    "sim_teddybear = [0.6466, 0.6668]\n",
    "\n",
    "dog_0_0 = [0.6241, 0.6054, 0.6097, 0.608, 0.6343]\n",
    "dog_01_0 = [0.5841, 0.5235, 0.5014, 0.51, 0.5088]\n",
    "dog_01_0_onlyK = [0.6115, 0.5835, 0.5924, 0.5809]\n",
    "sim_dog = [0.5982, 0.6258]\n",
    "\n",
    "plushy_01_0_onlyK = [0.807, 0.7913, 0.7771, 0.7607, 0.7532]\n",
    "plushy_01_0_onlyV = [0.8067, 0.79, 0.7706, 0.754, 0.7436]\n",
    "sim_plushy = [0.7917, 0.802]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def draw_line2(x1,x2 =None,x3= None,x4=None,constant1=None,constant2=None):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x1, marker='*', linestyle='-', color='r',label = 'no reg.')\n",
    "    if x2 is not None:\n",
    "        ax.plot(x2, marker='o', linestyle='-', color='g',label = '0.1 text reg.')\n",
    "    if x3 is not None:\n",
    "        ax.plot(x3, marker='s', linestyle='-', color='b',label = '1.0 text reg.')\n",
    "    if x4 is not None:\n",
    "        ax.plot(x4, marker='s', linestyle='--', color='g',label = '<new>, reg')\n",
    "    if constant1 is not None:\n",
    "        # draw a horizontal line\n",
    "        ax.axhline(y=constant1, color='r', linestyle='--',label = 'real reg.')\n",
    "    if constant2 is not None:\n",
    "        # draw a horizontal line\n",
    "        ax.axhline(y=constant2, color='y', linestyle='--',label = 'gen. reg.')\n",
    "        \n",
    "    plt.legend( loc='best',fontsize=10)\n",
    "    plt.xticks(np.arange(0,len(x1) +1),fontsize=7)\n",
    "    plt.show()\n",
    "draw_line2(cat_0_0,cat_01_0,cat_10_0,constant1 = sim_cat[0],constant2 = sim_cat[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dog_epoch = [0.5962,0.6115,0.5835,0.5924,0.5809]\n",
    "sim_dog = [0.5982, 0.6258]\n",
    "\n",
    "sim_tortoise_plushy_epoch = [0.8040, 0.7401,0.6886,0.6640,0.6285,0.6267]\n",
    "\n",
    "sim_cat_epoch = [0.7061, 0.7401,0.6886,0.6640,0.6285,0.6267]\n",
    "sim_cat = [0.7011, 0.6720, 0.6267]\n",
    "\n",
    "sim_teddybear_epoch = [0.7455, 0.7401,0.6886,0.6640,0.6285,0.6267]\n",
    "sim_teddybear = [0.7611, 0.7369, 0.6267]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "str = \"2023-04-05T10-16-22_tortoise_plushyreg_1_scale1_a_tortoise_plushy_ridge\"\n",
    "matchObj = re.match(r'.*_(.*)reg.*a_(.*)_(.*)', str, re.M|re.I)\n",
    "\n",
    "if matchObj:\n",
    "   print(\"matchObj.group(1) : \", matchObj.group(1))\n",
    "   print(\"matchObj.group(2) : \", matchObj.group(2))\n",
    "   print(\"matchObj.group(3) : \", matchObj.group(3))\n",
    "else:\n",
    "   print(\"No match!!\")\n",
    "\n",
    "file = 'logs/2023-04-05T15-42-14_catreg_0.1_scale0.1_a_cat_ridge/checkpoints/epoch=000011-step=000000399.ckpt'\n",
    "out = re.match(r'.*step=0+(\\d+).ckpt',file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "dirs = ['logs/logs_lds11/2023-04-12T09-01-16_tortoise_plushy_reg0.1-0.1_scale0-0', 'logs/logs_lds11/2023-04-12T08-49-20_tortoise_plushy_classBias_reg0.1-0.1_scale0-0', 'logs/logs_lds11/2023-04-12T04-10-18_tortoise_plushy_realReg__reg0.1-0.1_scale0-0','logs/logs_lds11/2023-04-12T05-53-28_tortoise_plushy_realReg_classBias_reg0.1-0.1_scale0-0', 'logs/logs_liox/2023-04-05T14-20-45_tortoise_plushy-sdv4','logs/logs_liox/2023-04-06T15-39-11_tortoise_plushy-sdv4']\n",
    "profix = ['delta_epoch=last','delta_epoch=last','delta_epoch=last','last','delta_epoch=last','delta_epoch=last']\n",
    "output_dir = 'output/tortoise_Classbias/'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for filename in os.listdir(dirs[0]):\n",
    "    if filename.endswith('.png') and filename.startswith(profix[0]):\n",
    "        filename = filename.replace(profix[0],'')\n",
    "        try:\n",
    "            # print([(d + '/' + ''.join([p,filename])) for d,p in zip(dirs,profix)])\n",
    "            images = [Image.open(d + '/' + ''.join([p,filename])) for d,p in zip(dirs,profix)]\n",
    "        except:\n",
    "            print(\"error\",filename)\n",
    "            continue\n",
    "        widths, heights = zip(*(i.size for i in images))\n",
    "        total_width = max(widths)\n",
    "        max_height = sum(heights)\n",
    "        new_im = Image.new('RGB', (total_width, max_height))\n",
    "        x_offset = 0\n",
    "        for im in images:\n",
    "            new_im.paste(im, (0, x_offset))\n",
    "            x_offset += im.size[1]\n",
    "        new_im.save(os.path.join(output_dir, filename))\n",
    "        print(\"saved\",filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "dirs = ['logs/logs_lds11/2023-04-12T08-10-40_cat_reg0.1-0.1_scale0-0', 'logs/logs_lds11/2023-04-12T07-56-34_cat_classBias_reg0.1-0.1_scale0-0', 'logs/logs_lds11/2023-04-12T03-25-09_cat_realReg__reg0.1-0.1_scale0-0', 'logs/logs_lds11/2023-04-12T05-30-17_cat_realReg_classBias_reg0.1-0.1_scale0-0','logs/logs_liox/2023-04-05T05-04-21_cat-sdv4','logs/logs_liox/2023-04-06T15-07-33_cat-sdv4']\n",
    "profix = ['delta_epoch=last','delta_epoch=last','delta_epoch=last','last','delta_epoch=last','delta_epoch=last']\n",
    "output_dir = 'output/cat_Classbias/'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for filename in os.listdir(dirs[0]):\n",
    "    if filename.endswith('.png') and filename.startswith(profix[0]):\n",
    "        filename = filename.replace(profix[0],'')\n",
    "        try:\n",
    "            # print([(d + '/' + ''.join([p,filename])) for d,p in zip(dirs,profix)])\n",
    "            images = [Image.open(d + '/' + ''.join([p,filename])) for d,p in zip(dirs,profix)]\n",
    "        except:\n",
    "            print(\"error\",filename)\n",
    "            continue\n",
    "        widths, heights = zip(*(i.size for i in images))\n",
    "        total_width = max(widths)\n",
    "        max_height = sum(heights)\n",
    "        new_im = Image.new('RGB', (total_width, max_height))\n",
    "        x_offset = 0\n",
    "        for im in images:\n",
    "            new_im.paste(im, (0, x_offset))\n",
    "            x_offset += im.size[1]\n",
    "        new_im.save(os.path.join(output_dir, filename))\n",
    "        print(\"saved\",filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = ['logs/logs_lds11/2023-04-12T08-36-57_teddybear_reg0.1-0.1_scale0-0', 'logs/logs_lds11/2023-04-12T08-24-32_teddybear_classBias_reg0.1-0.1_scale0-0', 'logs/logs_lds11/2023-04-12T03-47-36_teddybear_realReg__reg0.1-0.1_scale0-0','logs/logs_lds11/2023-04-12T05-42-56_teddybear_realReg_classBias_reg0.1-0.1_scale0-0', 'logs/logs_liox/2023-04-05T05-15-20_teddybear-sdv4','logs/logs_liox/2023-04-06T15-24-25_teddybear-sdv4']\n",
    "profix = ['delta_epoch=last','delta_epoch=last','delta_epoch=last','last','delta_epoch=last','delta_epoch=last']\n",
    "output_dir = 'output/teddybear_classBias/'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for filename in os.listdir(dirs[0]):\n",
    "    if filename.endswith('.png') and filename.startswith(profix[0]):\n",
    "        filename = filename.replace(profix[0],'')\n",
    "        try:\n",
    "            # print([(d + '/' + ''.join([p,filename])) for d,p in zip(dirs,profix)])\n",
    "            images = [Image.open(d + '/' + ''.join([p,filename])) for d,p in zip(dirs,profix)]\n",
    "        except:\n",
    "            print(\"error\",filename)\n",
    "            continue\n",
    "        widths, heights = zip(*(i.size for i in images))\n",
    "        total_width = max(widths)\n",
    "        max_height = sum(heights)\n",
    "        new_im = Image.new('RGB', (total_width, max_height))\n",
    "        x_offset = 0\n",
    "        for im in images:\n",
    "            new_im.paste(im, (0, x_offset))\n",
    "            x_offset += im.size[1]\n",
    "        new_im.save(os.path.join(output_dir, filename))\n",
    "        print(\"saved\",filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the grid\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.linspace(-5, 5, 100)\n",
    "x, y = np.meshgrid(x, y)\n",
    "\n",
    "# Define the horizontal plane\n",
    "z1 = np.zeros_like(x)\n",
    "z1.fill(1.1)\n",
    "\n",
    "# Define the Gaussian plane\n",
    "z2 = np.exp(-(x**2 + y**2)/10)+0.7\n",
    "# z3 = np.exp(-(x**2 + y**2)/0.02)\n",
    "\n",
    "z2  = z2 \n",
    "\n",
    "# Calculate the volume under the Gaussian plane\n",
    "# vol_z2 = np.trapz(np.trapz(z2, x[0,:]), y[:,0])\n",
    "\n",
    "# Adjust the height of the horizontal plane to match the volume under the Gaussian plane\n",
    "# z1 = z1 + vol_z2 / (x.shape[0] * y.shape[1])\n",
    "\n",
    "# Plot the planes\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(x, y, z1, alpha =0.5)\n",
    "ax.plot_surface(x, y, z2, alpha =0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13594a93d8c6cfa5486524ed2a59c700c6ea0db7c00ff6bfcf5e58f97c37b64a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
